{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cyanite Audio Analyzer SageMaker Inference Demo\n",
    "\n",
    "This notebook demonstrates how to deploy and call the Cyanite Audio Analyzer marketplace algorithm using the Sagemaker Python SDK.\n",
    "\n",
    "Here, we will only cover a basic usage scenario. For more information on advanced functionality please refer to the [Python Sagemaker SKD Documentation](https://sagemaker.readthedocs.io/en/stable/index.html) and the [Amazon Sagemaker API Reference](https://docs.aws.amazon.com/sagemaker/latest/APIReference/Welcome.html).\n",
    "\n",
    "__Please Note__: In order to follow this notebook you have to be subscribed to the Cyanite Audio Analyzer on the AWS Marketplace.\n",
    "\n",
    "Contents:\n",
    "- [Real-time Inference](#rtinference)\n",
    "- [Asyncronous Inference](#asyncinference)\n",
    "- [Batch Transformation Jobs](#batchtransform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install latest SageMaker SDK\n",
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites\n",
    "\n",
    "Set up necessary variables. \n",
    "\n",
    "***Important***: \n",
    "\n",
    "- Please replace `model_arn` with the ARN of your Audio Analyzer algorithm subscription. \n",
    "\n",
    "- Replace `s3_bucket` with your s3 bucket name and `base_prefix` with the prefix where you want to store outputs of the asynchronous inference and batch transformation job. Make sure that your `role` has read/write rights for the chosen bucket.\n",
    "\n",
    "Optionally, you can set your own `audio_filepath`. Supported file formats are _mp3_, _wav_, _m4a_, _mp4_ and _mpga_. \n",
    "\n",
    "You can also choose a  different `base_name` or manually set the IAM `role`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the ARN of you Audio Analyzer algorithm subscripiton\n",
    "model_arn = \"your-arn-here\"\n",
    "\n",
    "# base name for endpoint deployment\n",
    "base_name = \"cyanite-analyzer-demo\"\n",
    "\n",
    "# bucket for storing async and transformation results\n",
    "s3_bucket = \"your-bucket-here\"\n",
    "\n",
    "# path on bucket, where to store async and transformation results\n",
    "base_prefix = \"your-prefix-here\"\n",
    "\n",
    "# Path to audio files\n",
    "audio_filepath = \"example-audios\"\n",
    "\n",
    "# supported file formats\n",
    "file_formats = [\"mp3\", \"wav\", \"m4a\", \"mp4\", \"mpga\"]\n",
    "\n",
    "# Replace this with a valid IAM role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Sagemaker Session\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio files:\n",
      "['example-audios/podcast-jingle-01-by-musikhalde-from-filmmusic-io.mp3']\n"
     ]
    }
   ],
   "source": [
    "# define job/endpoint names\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "rt_endpoint_name = f\"{base_name}-rt-{timestamp}\"\n",
    "async_endpoint_name = f\"{base_name}-async-{timestamp}\"\n",
    "transform_job_name = f\"{base_name}-transformer-{timestamp}\"\n",
    "\n",
    "# gather available audio files\n",
    "available_files = [\n",
    "    xx for x in file_formats for xx in pathlib.Path(audio_filepath).rglob(f\"*.{x}\")\n",
    "]\n",
    "print(f\"Available audio files:\\n{[x.as_posix() for x in available_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare SageMaker Model\n",
    "\n",
    "First we instantiate the SageMaker model from your provided Model Package ARN.\n",
    "\n",
    "This model object is the starting point for all three deployment types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SageMaker model from marketplace model package\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_arn,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rtinference\"></a>\n",
    "## 3. Real-time Inference\n",
    "\n",
    "In the real-time inference scenario, our algorithm is deployed on an EC2 instance. \n",
    "We can then send audio data directly to the deployed endpoint using REST calls. \n",
    "The endpoint will handle the request and send the results back to the caller. \n",
    "\n",
    "***Important:***\n",
    "- Real-time inference requests cannot exceed a payload size of 6MB\n",
    "- Inference requests will time out after 60 seconds. Depending on the chose EC2 instance and the length of the audio, processing time might exceed that limit. In this case, please refer to another inference type.\n",
    "\n",
    "### Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying: cyanite-analyzer-demo-rt-10-11-2022-21-09-28\n",
      "---------!"
     ]
    }
   ],
   "source": [
    "print(f\"Deploying: {rt_endpoint_name}\")\n",
    "\n",
    "model.deploy(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=rt_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor instance for endpoint\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    rt_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: podcast-jingle-01-by-musikhalde-from-filmmusic-io.mp3\n",
      "Output stored at results/podcast-jingle-01-by-musikhalde-from-filmmusic-io.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to store analysis results\n",
    "results_filepath = \"results\"\n",
    "\n",
    "# create results filepath\n",
    "pathlib.Path(results_filepath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for path in available_files:\n",
    "\n",
    "    print(f\"Processing: {path.name}\")\n",
    "\n",
    "    # load audio as binary\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_audio_data = f.read()\n",
    "\n",
    "    # call endpoint\n",
    "    out_json = predictor.predict(data=raw_audio_data)\n",
    "\n",
    "    # store result\n",
    "    out_path = (\n",
    "        path.as_posix().replace(audio_filepath, results_filepath).replace(\"mp3\", \"json\")\n",
    "    )\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(json.dumps(out_json))\n",
    "\n",
    "    print(f\"Output stored at {out_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# display latest processing result\n",
    "ipd.JSON(json.load(open(out_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after usage, we can delete the enpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"asyncinference\"></a>\n",
    "## 4. Asynchronous Inference\n",
    "\n",
    "In the asynchronous inference scenario, our algorithm is deployed on an EC2 instance. \n",
    "Instead of calling the deployed endpoint directly, we send the input data to S3 for intermediary storage. \n",
    "AWS will handle the data using an internal queue and will do calls to the endpoint when it is available. \n",
    "Results will again be stored on S3 where we can fetch them.\n",
    "\n",
    "***Important:***\n",
    "- Asynchronous inference requests can be up to 500MB in size\n",
    "- requests will time out after 15 minutes \n",
    "\n",
    "### Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up s3 paths\n",
    "async_input_prefix = f\"{base_prefix}/async_inputs\"\n",
    "async_output_prefix = f\"{base_prefix}/async_outputs\"\n",
    "\n",
    "# create async config\n",
    "# here we define, where on s3 the results will be stored\n",
    "async_config = sagemaker.async_inference.async_inference_config.AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{s3_bucket}/{async_output_prefix}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying: cyanite-analyzer-demo-async-10-11-2022-21-09-28\n",
      "---------!"
     ]
    }
   ],
   "source": [
    "print(f\"Deploying: {async_endpoint_name}\")\n",
    "\n",
    "model.deploy(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    async_inference_config=async_config,\n",
    "    endpoint_name=async_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictor instance for an asynchronous endpoint\n",
    "async_predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "    sagemaker.predictor.Predictor(\n",
    "        async_endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: podcast-jingle-01-by-musikhalde-from-filmmusic-io.mp3\n",
      "Waiting for results to be available..\n",
      "Output stored at results_async/podcast-jingle-01-by-musikhalde-from-filmmusic-io.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# local path to store analysis results\n",
    "results_filepath = \"results_async\"\n",
    "\n",
    "# create results filepath\n",
    "pathlib.Path(results_filepath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# process data\n",
    "for path in available_files:\n",
    "\n",
    "    print(f\"Processing: {path.name}\")\n",
    "\n",
    "    # load audio as binary\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_audio_data = f.read()\n",
    "\n",
    "    # call endpoint\n",
    "    async_response = async_predictor.predict_async(\n",
    "        data=raw_audio_data,\n",
    "        input_path=f\"s3://{s3_bucket}/{async_input_prefix}/{path.name}\",\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for results to be available..\")\n",
    "    out_json = async_response.get_result(\n",
    "        waiter_config=sagemaker.async_inference.waiter_config.WaiterConfig(\n",
    "            max_attempts=20, delay=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # store result\n",
    "    out_path = (\n",
    "        path.as_posix().replace(audio_filepath, results_filepath).replace(\"mp3\", \"json\")\n",
    "    )\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(json.dumps(out_json))\n",
    "\n",
    "    print(f\"Output stored at {out_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# display latest processing result\n",
    "ipd.JSON(json.load(open(out_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after usage, we can delete the enpoint\n",
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"batchtransform\"></a>\n",
    "## 5. Batch Transformation Jobs\n",
    "\n",
    "Batch transformation jobs can be utilized in case we have an one-time batch of data to be processed. \n",
    "A batch transformation job will deploy our algorithm on an EC2 instance, process our pre-defined data batch and shut down afterwards. \n",
    "\n",
    "To start a transformation job, we will need to provide a S3 location containing all data to be processed. \n",
    "Alternatively we can provide the transformer with a manifest file which points to the locations of data to be processed (not covered here).\n",
    "\n",
    "Results will again be stored on S3 where we can fetch them.\n",
    "\n",
    "***Important:***\n",
    "- The maximum data size per file cannot exceed 100MB \n",
    "- there is no timeout limit for request processing\n",
    "\n",
    "\n",
    "### Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up s3 paths\n",
    "batch_input_prefix = f\"{base_prefix}/batch_inputs\"\n",
    "batch_output_prefix = f\"{base_prefix}/batch_outputs\"\n",
    "\n",
    "# instantiate uploader\n",
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "# upload audios to be processed\n",
    "for path in available_files:\n",
    "    s3_uploader.upload(\n",
    "        local_path=path.as_posix(),\n",
    "        desired_s3_uri=f\"s3://{s3_bucket}/{batch_input_prefix}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize transformer\n",
    "batch_transformer = model.transformer(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    strategy=\"SingleRecord\",\n",
    "    max_concurrent_transforms=1,\n",
    "    max_payload=100,\n",
    "    output_path=f\"s3://{s3_bucket}/{batch_output_prefix}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transform job: cyanite-analyzer-demo-transformer-10-11-2022-21-09-28\n",
      "........................................\u001b[34mStarting the inference server with 1 worker(s).\u001b[0m\n",
      "\u001b[34mInternal worker timeout set to 900s.\u001b[0m\n",
      "\u001b[34m[2022-11-10 21:25:21 +0000] [10] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-11-10 21:25:21 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\u001b[0m\n",
      "\u001b[34m[2022-11-10 21:25:21 +0000] [10] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-11-10 21:25:21 +0000] [13] [INFO] Booting worker with pid: 13\u001b[0m\n",
      "\u001b[34m2022-11-10 21:25:32.275342: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (34)\u001b[0m\n",
      "\u001b[34m2022-11-10 21:25:32.275390: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (45f116645c2d): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2022-11-10 21:25:32.275679: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/scipy/sparse/data.py:74: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  self._deduped_data().astype(dtype, casting=casting, copy=copy),\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\u001b[0m\n",
      "\u001b[34mGPU AVAILABLE? False\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/Nov/2022:21:25:51 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/Nov/2022:21:25:51 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mGPU AVAILABLE? False\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/Nov/2022:21:25:51 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/Nov/2022:21:25:51 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2022-11-10T21:25:51.634:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=100, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/Nov/2022:21:25:57 +0000] \"POST /invocations HTTP/1.1\" 200 17530 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/Nov/2022:21:25:57 +0000] \"POST /invocations HTTP/1.1\" 200 17530 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting transform job: {transform_job_name}\")\n",
    "\n",
    "batch_transformer.transform(\n",
    "    job_name=transform_job_name,\n",
    "    data=f\"s3://{s3_bucket}/{batch_input_prefix}\",\n",
    "    data_type=\"S3Prefix\",\n",
    "    split_type=None,\n",
    "    content_type=None,\n",
    "    compression_type=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local path to store analysis results\n",
    "results_filepath = \"results_transform\"\n",
    "\n",
    "# instantiate downloader\n",
    "s3_downloader = sagemaker.s3.S3Downloader()\n",
    "\n",
    "s3_downloader.download(\n",
    "    s3_uri=f\"s3://{s3_bucket}/{batch_output_prefix}\",\n",
    "    local_path=results_filepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "transform_results = [x for x in pathlib.Path(results_filepath).rglob(\"*\")]\n",
    "\n",
    "# display first result\n",
    "# please note, that the default file ending for batch transformation\n",
    "# outputs is .out\n",
    "ipd.JSON(json.load(open(transform_results[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tips\n",
    "- Choose the `instance_type` according to your requirements. For best runtime, choose a GPU accelerated machine. See [here](https://aws.amazon.com/sagemaker/pricing/) for more information\n",
    "- You can scale the amount of machines used for your endpoint/transform jobs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyze_v3",
   "language": "python",
   "name": "analyze_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
