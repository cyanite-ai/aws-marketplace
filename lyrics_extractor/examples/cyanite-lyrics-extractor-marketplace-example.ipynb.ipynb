{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cyanite Lyrics Extractor SageMaker Inference Demo\n",
    "\n",
    "This notebook demonstrates how to deploy and call the Cyanite Lyrics Extractor marketplace algorithm using the Sagemaker Python SDK.\n",
    "\n",
    "Here, we will only cover a basic usage scenario. For more information on advanced functionality please refer to the [Python Sagemaker SKD Documentation](https://sagemaker.readthedocs.io/en/stable/index.html) and the [Amazon Sagemaker API Reference](https://docs.aws.amazon.com/sagemaker/latest/APIReference/Welcome.html).\n",
    "\n",
    "__Please Note__: In order to follow this notebook you have to be subscribed to the Cyanite Audio Analyzer on the AWS Marketplace.\n",
    "\n",
    "Contents:\n",
    "- [Real-time Inference](#rtinference)\n",
    "- [Asyncronous Inference](#asyncinference)\n",
    "- [Batch Transformation Jobs](#batchtransform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install latest SageMaker SDK\n",
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites\n",
    "\n",
    "Set up necessary variables. \n",
    "\n",
    "***Important***: \n",
    "\n",
    "- Please replace `model_arn` with the ARN of your Audio Analyzer algorithm subscription. \n",
    "\n",
    "- Replace `s3_bucket` with your s3 bucket name and `base_prefix` with the prefix where you want to store outputs of the asynchronous inference and batch transformation job. Make sure that your `role` has read/write rights for the chosen bucket.\n",
    "\n",
    "Optionally, you can set your own `audio_filepath`. Supported file formats are _mp3_, _wav_, _m4a_, _mp4_ and _mpga_. \n",
    "\n",
    "You can also choose a  different `base_name` or manually set the IAM `role`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the ARN of you Audio Analyzer algorithm subscripiton\n",
    "model_arn = \"your-arn-here\"\n",
    "\n",
    "# base name for endpoint deployment\n",
    "base_name = \"cyanite-lyrics-extractor-demo\"\n",
    "\n",
    "# bucket for storing async and transformation results\n",
    "s3_bucket = \"your-bucket-here\"\n",
    "\n",
    "# path on bucket, where to store async and transformation results\n",
    "base_prefix = \"your-prefix-here\"\n",
    "\n",
    "# Path to audio files\n",
    "audio_filepath = \"example-audios\"\n",
    "\n",
    "# supported file formats\n",
    "file_formats = [\"mp3\", \"wav\", \"m4a\", \"mp4\", \"mpga\"]\n",
    "\n",
    "# Replace this with a valid IAM role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Sagemaker Session\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio files:\n",
      "['example-audios/undamus_30s.mp3']\n"
     ]
    }
   ],
   "source": [
    "# define job/endpoint names\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "rt_endpoint_name = f\"{base_name}-rt-{timestamp}\"\n",
    "async_endpoint_name = f\"{base_name}-async-{timestamp}\"\n",
    "transform_job_name = f\"{base_name}-transformer-{timestamp}\"\n",
    "\n",
    "# gather available audio files\n",
    "available_files = [\n",
    "    xx for x in file_formats for xx in pathlib.Path(audio_filepath).rglob(f\"*.{x}\")\n",
    "]\n",
    "print(f\"Available audio files:\\n{[x.as_posix() for x in available_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare SageMaker Model\n",
    "\n",
    "First we instantiate the SageMaker model from your provided Model Package ARN.\n",
    "\n",
    "This model object is the starting point for all three deployment types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SageMaker model from marketplace model package\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_arn,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rtinference\"></a>\n",
    "## 3. Real-time Inference\n",
    "\n",
    "In the real-time inference scenario, our algorithm is deployed on an EC2 instance. \n",
    "We can then send audio data directly to the deployed endpoint using REST calls. \n",
    "The endpoint will handle the request and send the results back to the caller. \n",
    "\n",
    "***Important:***\n",
    "- Real-time inference requests cannot exceed a payload size of 6MB\n",
    "- Inference requests will time out after 60 seconds. Depending on the chose EC2 instance and the length of the audio, processing time might exceed that limit. In this case, please refer to another inference type.\n",
    "\n",
    "### Deploy Endpoint\n",
    "\n",
    "Deployment may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying: cyanite-lyrics-extractor-demo-rt-22-11-2022-14-51-23\n",
      "-----------!"
     ]
    }
   ],
   "source": [
    "print(f\"Deploying: {rt_endpoint_name}\")\n",
    "\n",
    "model.deploy(\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=rt_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor instance for endpoint\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    rt_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: undamus_30s.mp3\n",
      "Output stored at results/undamus_30s.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to store analysis results\n",
    "results_filepath = \"results\"\n",
    "\n",
    "# create results filepath\n",
    "pathlib.Path(results_filepath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for path in available_files:\n",
    "\n",
    "    print(f\"Processing: {path.name}\")\n",
    "\n",
    "    # load audio as binary\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_audio_data = f.read()\n",
    "\n",
    "    # call endpoint\n",
    "    out_json = predictor.predict(data=raw_audio_data)\n",
    "\n",
    "    # store result\n",
    "    out_path = (\n",
    "        path.as_posix().replace(audio_filepath, results_filepath).replace(\"mp3\", \"json\")\n",
    "    )\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(json.dumps(out_json))\n",
    "\n",
    "    print(f\"Output stored at {out_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "extractedLyrics": " When the dawn starts to creep Back into the sky And it's time to leave We can still feel the drum And it's pulling us right back to where we've begun"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# display latest processing result\n",
    "ipd.JSON(json.load(open(out_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after usage, we can delete the enpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"asyncinference\"></a>\n",
    "## 4. Asynchronous Inference\n",
    "\n",
    "In the asynchronous inference scenario, our algorithm is deployed on an EC2 instance. \n",
    "Instead of calling the deployed endpoint directly, we send the input data to S3 for intermediary storage. \n",
    "AWS will handle the data using an internal queue and will do calls to the endpoint when it is available. \n",
    "Results will again be stored on S3 where we can fetch them.\n",
    "\n",
    "***Important:***\n",
    "- Asynchronous inference requests can be up to 500MB in size\n",
    "- requests will time out after 15 minutes \n",
    "\n",
    "### Deploy Endpoint\n",
    "\n",
    "Deployment may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up s3 paths\n",
    "async_input_prefix = f\"{base_prefix}/async_inputs\"\n",
    "async_output_prefix = f\"{base_prefix}/async_outputs\"\n",
    "\n",
    "# create async config\n",
    "# here we define, where on s3 the results will be stored\n",
    "async_config = sagemaker.async_inference.async_inference_config.AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{s3_bucket}/{async_output_prefix}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying: cyanite-lyrics-extractor-demo-async-22-11-2022-14-51-23\n",
      "----------!"
     ]
    }
   ],
   "source": [
    "print(f\"Deploying: {async_endpoint_name}\")\n",
    "\n",
    "model.deploy(\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    async_inference_config=async_config,\n",
    "    endpoint_name=async_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictor instance for an asynchronous endpoint\n",
    "async_predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "    sagemaker.predictor.Predictor(\n",
    "        async_endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: undamus_30s.mp3\n",
      "Waiting for results to be available..\n",
      "Output stored at results_async/undamus_30s.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# local path to store analysis results\n",
    "results_filepath = \"results_async\"\n",
    "\n",
    "# create results filepath\n",
    "pathlib.Path(results_filepath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# process data\n",
    "for path in available_files:\n",
    "\n",
    "    print(f\"Processing: {path.name}\")\n",
    "\n",
    "    # load audio as binary\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_audio_data = f.read()\n",
    "\n",
    "    # call endpoint\n",
    "    async_response = async_predictor.predict_async(\n",
    "        data=raw_audio_data,\n",
    "        input_path=f\"s3://{s3_bucket}/{async_input_prefix}/{path.name}\",\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for results to be available..\")\n",
    "    out_json = async_response.get_result(\n",
    "        waiter_config=sagemaker.async_inference.waiter_config.WaiterConfig(\n",
    "            max_attempts=20, delay=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # store result\n",
    "    out_path = (\n",
    "        path.as_posix().replace(audio_filepath, results_filepath).replace(\"mp3\", \"json\")\n",
    "    )\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(json.dumps(out_json))\n",
    "\n",
    "    print(f\"Output stored at {out_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "extractedLyrics": " When the dawn starts to creep Back into the sky And it's time to leave We can still feel the drum And it's pulling us right back to where we've begun"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# display latest processing result\n",
    "ipd.JSON(json.load(open(out_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after usage, we can delete the enpoint\n",
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"batchtransform\"></a>\n",
    "## 5. Batch Transformation Jobs\n",
    "\n",
    "Batch transformation jobs can be utilized in case we have an one-time batch of data to be processed. \n",
    "A batch transformation job will deploy our algorithm on an EC2 instance, process our pre-defined data batch and shut down afterwards. \n",
    "\n",
    "To start a transformation job, we will need to provide a S3 location containing all data to be processed. \n",
    "Alternatively we can provide the transformer with a manifest file which points to the locations of data to be processed (not covered here).\n",
    "\n",
    "Results will again be stored on S3 where we can fetch them.\n",
    "\n",
    "***Important:***\n",
    "- The maximum data size per file cannot exceed 100MB \n",
    "- there is no timeout limit for request processing\n",
    "\n",
    "### Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up s3 paths\n",
    "batch_input_prefix = f\"{base_prefix}/batch_inputs\"\n",
    "batch_output_prefix = f\"{base_prefix}/batch_outputs\"\n",
    "\n",
    "# instantiate uploader\n",
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "# upload audios to be processed\n",
    "for path in available_files:\n",
    "    s3_uploader.upload(\n",
    "        local_path=path.as_posix(),\n",
    "        desired_s3_uri=f\"s3://{s3_bucket}/{batch_input_prefix}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize transformer\n",
    "batch_transformer = model.transformer(\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    strategy=\"SingleRecord\",\n",
    "    max_concurrent_transforms=1,\n",
    "    max_payload=100,\n",
    "    output_path=f\"s3://{s3_bucket}/{batch_output_prefix}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Before executing the next step: \n",
      "Please be aware that batch inference jobs have a cold start period of up to several minutes that is being billed. \n",
      "To mitigate this cost, batch transformation jobs should be used for bigger batches of data only. \n",
      "Input 'ok' to continue..\n",
      " ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transform job: cyanite-lyrics-extractor-demo-transformer-22-11-2022-14-51-23\n",
      "..............................................\u001b[34mStarting the inference server with 1 workers.\u001b[0m\n",
      "\u001b[34mInternal worker timeout set to 900s.\u001b[0m\n",
      "\u001b[34m[2022-11-22 15:14:26 +0000] [10] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-11-22 15:14:26 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\u001b[0m\n",
      "\u001b[34m[2022-11-22 15:14:26 +0000] [10] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-11-22 15:14:26 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[34mcpu\u001b[0m\n",
      "\u001b[35mStarting the inference server with 1 workers.\u001b[0m\n",
      "\u001b[35mInternal worker timeout set to 900s.\u001b[0m\n",
      "\u001b[35m[2022-11-22 15:14:26 +0000] [10] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[35m[2022-11-22 15:14:26 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\u001b[0m\n",
      "\u001b[35m[2022-11-22 15:14:26 +0000] [10] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2022-11-22 15:14:26 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[35mcpu\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Nov/2022:15:14:30 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Nov/2022:15:14:30 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Nov/2022:15:14:30 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Nov/2022:15:14:30 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.9/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Nov/2022:15:14:32 +0000] \"POST /invocations HTTP/1.1\" 200 49 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Nov/2022:15:14:32 +0000] \"POST /invocations HTTP/1.1\" 200 49 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.9/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\u001b[0m\n",
      "\u001b[32m2022-11-22T15:14:30.283:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=100, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Before executing the next step: \\nPlease be aware that batch inference jobs have a cold start period of up to several minutes that is being billed. \\nTo mitigate this cost, batch transformation jobs should be used for bigger batches of data only. \\nInput 'ok' to continue..\\n\")\n",
    "\n",
    "if prompt == \"ok\":\n",
    "\n",
    "    print(f\"Starting transform job: {transform_job_name}\")\n",
    "\n",
    "    batch_transformer.transform(\n",
    "        job_name=transform_job_name,\n",
    "        data=f\"s3://{s3_bucket}/{batch_input_prefix}\",\n",
    "        data_type=\"S3Prefix\",\n",
    "        split_type=None,\n",
    "        content_type=None,\n",
    "        compression_type=None,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"Batch transform job not started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local path to store analysis results\n",
    "results_filepath = \"results_transform\"\n",
    "\n",
    "# instantiate downloader\n",
    "s3_downloader = sagemaker.s3.S3Downloader()\n",
    "\n",
    "s3_downloader.download(\n",
    "    s3_uri=f\"s3://{s3_bucket}/{batch_output_prefix}\",\n",
    "    local_path=results_filepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "extractedLyrics": " Lights, camera, Hollywood"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "transform_results = [x for x in pathlib.Path(results_filepath).rglob(\"*\")]\n",
    "\n",
    "# display first result\n",
    "# please note, that the default file ending for batch transformation\n",
    "# outputs is .out\n",
    "ipd.JSON(json.load(open(transform_results[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tips\n",
    "- Choose the `instance_type` according to your requirements. For best runtime, choose a GPU accelerated machine. See [here](https://aws.amazon.com/sagemaker/pricing/) for more information\n",
    "- You can scale the amount of machines used for your endpoint/transform jobs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyze_v3",
   "language": "python",
   "name": "analyze_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
